# AWS_Data-Engineering-Full-Stack-Sales-Data-Hudi-Glue-EMR-ManageAirflow

Sales Data Pipeline 
AWS Cloud Data Engineering Project | Data Pipeline using Apache Hudi, EMR, Glue Pyspark, Redshift, Athena, Manage Airflow

I want to share my latest Data Engineering project where I put my skills to the test by working with an AWS cloud

ğŸ”¬ğ—£ğ—¿ğ—¼ğ—·ğ—²ğ—°ğ˜ ğ—¢ğ˜ƒğ—²ğ—¿ğ˜ƒğ—¶ğ—²ğ˜„: End-to-end data engineering on AWS. Where I ingested data from file to S3 using Glue Pyspark, EMR and Redshift. Also used IAM and VPC for data governance. More information can be found in the GitHub repository.

ğŸ’¾ ğ——ğ—®ğ˜ğ—® ğ—¦ğ—¼ğ˜‚ğ—¿ğ—°ğ—²: 

ğŸ¯ ğ—£ğ—¿ğ—¼ğ—·ğ—²ğ—°ğ˜ ğ—šğ—¼ğ—®ğ—¹ğ˜€:

â€¢ Store data in S3.
â€¢ Run Glue Crawler and Pyspark (Bronze to silver) in Hudi Format
â€¢ Run EMR for ETL (Silver to Gold)
â€¢ Setup Redshift Cluster and ETL using Redshift only
â€¢ Steup Managed airflow to run all the step
â€¢ Implement IAM and VPC for governance.

 ğŸ”§The tools that are covered in this project are:

1. Amazon S3
2. Amazon Glue
3. Amazon IAM
4. Amazon VPC
5. Amaznon EMR
6. Amazon Managed Airflow
7. Amazon Redshift


## 1. Setup S3 bucket and load file

<img width="647" alt="image" src="https://github.com/user-attachments/assets/3bbba2f2-26fa-4699-8426-3adeb894ecf8" />
<img width="623" alt="image" src="https://github.com/user-attachments/assets/ac2c1d3d-3fa0-4047-b6ba-00092e838def" />

create raw folder

<img width="622" alt="image" src="https://github.com/user-attachments/assets/094f4247-db96-4533-9eb5-5316571d9c18" />

upload all file

<img width="630" alt="image" src="https://github.com/user-attachments/assets/40f5f34f-0ad3-496b-a7ad-803952a09ae5" />

## 2. Setup Glue Crawler and get data catalog



## 3. Run task using pyspark in glue (bronze to silver)

## 4. Setup EMR and run job (silver to gold)

## 4. Setup Redshfit cluster to run the task

## 5. Setup Managed Airflow to run all the step
