# AWS_Data-Engineering-Full-Stack-Sales-Data-Hudi-Glue-EMR-ManageAirflow

Sales Data Pipeline 
AWS Cloud Data Engineering Project | Data Pipeline using Apache Hudi, EMR, Glue Pyspark, Redshift, Athena, Manage Airflow

I want to share my latest Data Engineering project where I put my skills to the test by working with an AWS cloud

🔬𝗣𝗿𝗼𝗷𝗲𝗰𝘁 𝗢𝘃𝗲𝗿𝘃𝗶𝗲𝘄: End-to-end data engineering on AWS. Where I ingested data from file to S3 using Glue Pyspark, EMR and Redshift. Also used IAM and VPC for data governance. More information can be found in the GitHub repository.

💾 𝗗𝗮𝘁𝗮 𝗦𝗼𝘂𝗿𝗰𝗲: 

🎯 𝗣𝗿𝗼𝗷𝗲𝗰𝘁 𝗚𝗼𝗮𝗹𝘀:

• Store data in S3.
• Run Glue Crawler and Pyspark (Bronze to silver) in Hudi Format
• Run EMR for ETL (Silver to Gold)
• Setup Redshift Cluster and ETL using Redshift only
• Steup Managed airflow to run all the step
• Implement IAM and VPC for governance.

 🔧The tools that are covered in this project are:

1. Amazon S3
2. Amazon Glue
3. Amazon IAM
4. Amazon VPC
5. Amaznon EMR
6. Amazon Managed Airflow
7. Amazon Redshift


## 1. Setup S3 bucket and load file

<img width="647" alt="image" src="https://github.com/user-attachments/assets/3bbba2f2-26fa-4699-8426-3adeb894ecf8" />
<img width="623" alt="image" src="https://github.com/user-attachments/assets/ac2c1d3d-3fa0-4047-b6ba-00092e838def" />

create raw folder

<img width="622" alt="image" src="https://github.com/user-attachments/assets/094f4247-db96-4533-9eb5-5316571d9c18" />

upload all file

<img width="630" alt="image" src="https://github.com/user-attachments/assets/40f5f34f-0ad3-496b-a7ad-803952a09ae5" />

## 2. Setup Glue Crawler and get data catalog



## 3. Run task using pyspark in glue (bronze to silver)

## 4. Setup EMR and run job (silver to gold)

## 4. Setup Redshfit cluster to run the task

## 5. Setup Managed Airflow to run all the step
